import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import time

# Configuration
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
POSTGRES_DB = os.getenv("POSTGRES_DB", "energie_db")
POSTGRES_USER = os.getenv("POSTGRES_USER", "admin")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "admin123")
CASSANDRA_HOST = os.getenv("CASSANDRA_HOST", "cassandra")
CASSANDRA_PORT = int(os.getenv("CASSANDRA_PORT", "9042"))

print("=" * 70)
print(" POSTGRESQL ‚Üí SPARK STREAMING ‚Üí CASSANDRA")
print("=" * 70)

# Cr√©er la session Spark
spark = SparkSession.builder \
    .appName("PostgresToCassandraStreaming") \
    .config("spark.cassandra.connection.host", CASSANDRA_HOST) \
    .config("spark.cassandra.connection.port", CASSANDRA_PORT) \
    .config("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print(f" Spark Session cr√©√©e")
print(f" PostgreSQL: {POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}")
print(f"  Cassandra: {CASSANDRA_HOST}:{CASSANDRA_PORT}")

# URL de connexion PostgreSQL
jdbc_url = f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
connection_properties = {
    "user": POSTGRES_USER,
    "password": POSTGRES_PASSWORD,
    "driver": "org.postgresql.Driver"
}

# ====== FONCTION D'√âCRITURE DANS CASSANDRA ======
def write_to_cassandra(df, keyspace, table):
    """√âcrit un DataFrame dans Cassandra"""
    try:
        if df.count() > 0:
            df.write \
                .format("org.apache.spark.sql.cassandra") \
                .mode("append") \
                .options(table=table, keyspace=keyspace) \
                .save()
            print(f" {df.count()} lignes √©crites ‚Üí cassandra.{keyspace}.{table}")
            return True
        else:
            print(f"  Aucune nouvelle donn√©e")
            return False
    except Exception as e:
        print(f" Erreur √©criture Cassandra: {e}")
        import traceback
        traceback.print_exc()
        return False

# ====== FONCTION DE LECTURE INCR√âMENTALE DEPUIS POSTGRESQL ======
def get_last_processing_time_from_cassandra(table_name):
    """R√©cup√®re le dernier processing_time trait√© depuis Cassandra via ingestion_timestamp"""
    try:
        last_record = spark.read \
            .format("org.apache.spark.sql.cassandra") \
            .options(table=table_name, keyspace="energie") \
            .load() \
            .agg(max("ingestion_timestamp").alias("max_ingestion_timestamp")) \
            .collect()[0]["max_ingestion_timestamp"]
        
        if last_record:
            print(f" Derni√®re ingestion en Cassandra ({table_name}): {last_record}")
            return last_record
        else:
            print(f" Aucune donn√©e en Cassandra ({table_name}), chargement complet")
            return None
    except Exception as e:
        print(f"  Impossible de lire Cassandra ({table_name}): {e}")
        return None

def read_incremental_from_postgres(table_name, last_processing_time):
    """Lit les nouvelles donn√©es depuis PostgreSQL"""
    try:
        if last_processing_time:
            # Lecture incr√©mentale bas√©e sur processing_time
            query = f"(SELECT * FROM {table_name} WHERE processing_time > '{last_processing_time}' ORDER BY processing_time) AS incremental"
        else:
            # PREMI√àRE LECTURE: Charger toutes les donn√©es
            query = f"(SELECT * FROM {table_name} ORDER BY processing_time) AS full_load"
        
        df = spark.read.jdbc(
            url=jdbc_url,
            table=query,
            properties=connection_properties
        )
        
        count = df.count()
        if count > 0:
            print(f" {count} nouvelles lignes lues depuis postgres.{table_name}")
            # Afficher un √©chantillon
            df.select("zone", "datetime", "processing_time").show(5, truncate=False)
        else:
            print(f"  Aucune nouvelle ligne dans postgres.{table_name}")
        
        return df
    except Exception as e:
        print(f" Erreur lecture PostgreSQL ({table_name}): {e}")
        import traceback
        traceback.print_exc()
        return None

# ====== TRAITEMENT MIX √âNERG√âTIQUE ======
def process_mix_energie():
    """Traite et transf√®re mix_energie de PostgreSQL vers Cassandra"""
    print("\n" + "=" * 70)
    print(" Traitement: mix_energie_table (PostgreSQL ‚Üí Cassandra)")
    print("=" * 70)
    
    # R√©cup√©rer le dernier processing_time trait√©
    last_processing_time = get_last_processing_time_from_cassandra("mix_energie")
    
    # Lire depuis PostgreSQL
    df_postgres = read_incremental_from_postgres("mix_energie_table", last_processing_time)
    
    if df_postgres and df_postgres.count() > 0:
        # Transformer pour correspondre au sch√©ma Cassandra
        df_cassandra = df_postgres.select(
            col("zone").alias("region"),
            col("datetime").alias("timestamp"),
            coalesce(col("nuclear"), lit(0.0)).alias("nuclear"),
            coalesce(col("wind"), lit(0.0)).alias("wind"),
            coalesce(col("solar"), lit(0.0)).alias("solar"),
            coalesce(col("hydro"), lit(0.0)).alias("hydro"),
            coalesce(col("gas"), lit(0.0)).alias("gas"),
            coalesce(col("coal"), lit(0.0)).alias("coal"),
            coalesce(col("biomass"), lit(0.0)).alias("biomass"),
            lit(0.0).alias("geothermal"),  # Pas dans PostgreSQL
            lit(0.0).alias("oil"),  # Pas dans PostgreSQL
            coalesce(col("consumption_total"), lit(0.0)).alias("total_consumption"),
            coalesce(col("production_total"), lit(0.0)).alias("total_production"),
            lit(0.0).alias("total_import"),  # Pas dans PostgreSQL
            lit(0.0).alias("total_export"),  # Pas dans PostgreSQL
            coalesce(col("fossil_free_percentage"), lit(0.0)).alias("fossil_free_percentage"),
            coalesce(col("renewable_percentage"), lit(0.0)).alias("renewable_percentage"),
            col("processing_time").cast("string").alias("ingestion_timestamp"),
            lit("postgres-sync").alias("ingestion_id")
        )
        
        # Afficher un √©chantillon avant √©criture
        print(" Aper√ßu des donn√©es √† √©crire:")
        df_cassandra.select("region", "timestamp", "nuclear", "wind", "solar", "ingestion_timestamp").show(3, truncate=False)
        
        # √âcrire dans Cassandra
        return write_to_cassandra(df_cassandra, "energie", "mix_energie")
    else:
        print("  Aucune nouvelle donn√©e √† transf√©rer")
        return False

# ====== TRAITEMENT INTENSIT√â CARBONE ======
def process_carbone_energie():
    """Traite et transf√®re carbone_energie de PostgreSQL vers Cassandra"""
    print("\n" + "=" * 70)
    print(" Traitement: carbone_energie_table (PostgreSQL ‚Üí Cassandra)")
    print("=" * 70)
    
    # R√©cup√©rer le dernier processing_time trait√©
    last_processing_time = get_last_processing_time_from_cassandra("carbone_energie")
    
    # Lire depuis PostgreSQL
    df_postgres = read_incremental_from_postgres("carbone_energie_table", last_processing_time)
    
    if df_postgres and df_postgres.count() > 0:
        # Transformer pour correspondre au sch√©ma Cassandra
        df_cassandra = df_postgres.select(
            col("zone").alias("region"),
            col("datetime").alias("timestamp"),
            coalesce(col("carbon_intensity"), lit(0.0)).alias("carbon_intensity"),
            coalesce(col("emission_factor_type"), lit("unknown")).alias("emission_factor_type"),
            lit(False).alias("is_estimated"),  # Pas dans PostgreSQL
            lit("real_time").alias("estimation_method"),
            col("processing_time").cast("string").alias("ingestion_timestamp"),
            lit("postgres-sync").alias("ingestion_id")
        )
        
        # Afficher un √©chantillon avant √©criture
        print(" Aper√ßu des donn√©es √† √©crire:")
        df_cassandra.select("region", "timestamp", "carbon_intensity", "ingestion_timestamp").show(3, truncate=False)
        
        # √âcrire dans Cassandra
        return write_to_cassandra(df_cassandra, "energie", "carbone_energie")
    else:
        print("  Aucune nouvelle donn√©e √† transf√©rer")
        return False

# ====== TEST DE CONNEXION INITIAL ======
def test_connections():
    """Teste les connexions PostgreSQL et Cassandra"""
    print("\n" + "=" * 70)
    print("üîå TEST DES CONNEXIONS")
    print("=" * 70)
    
    # Test PostgreSQL
    try:
        df_test = spark.read.jdbc(
            url=jdbc_url,
            table="(SELECT COUNT(*) as count FROM mix_energie_table) AS test",
            properties=connection_properties
        )
        count = df_test.collect()[0]["count"]
        print(f" PostgreSQL: {count} enregistrements dans mix_energie_table")
    except Exception as e:
        print(f" PostgreSQL: Erreur de connexion - {e}")
        return False
    
    # Test Cassandra
    try:
        df_cassandra = spark.read \
            .format("org.apache.spark.sql.cassandra") \
            .options(table="mix_energie", keyspace="energie") \
            .load()
        count_cassandra = df_cassandra.count()
        print(f" Cassandra: {count_cassandra} enregistrements dans mix_energie")
    except Exception as e:
        print(f" Cassandra: Erreur de connexion - {e}")
        return False
    
    print("Toutes les connexions sont OK")
    return True

# ====== BOUCLE PRINCIPALE (STREAMING) ======
print("\n" + "=" * 70)
print(" D√âMARRAGE DU STREAMING POSTGRESQL ‚Üí CASSANDRA")
print("=" * 70)

# Test initial
if not test_connections():
    print("\n √âchec du test de connexion. Arr√™t du programme.")
    spark.stop()
    exit(1)

print("\ Intervalle de synchronisation: 30 secondes")
print(" Tables: mix_energie_table, carbone_energie_table")
print("\n En attente de donn√©es (Ctrl+C pour arr√™ter)...\n")

iteration = 0
try:
    while True:
        iteration += 1
        print(f"\n{'=' * 70}")
        print(f" IT√âRATION #{iteration} - {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        
        # Traiter mix_energie
        processed_mix = process_mix_energie()
        
        # Traiter carbone_energie
        processed_carbone = process_carbone_energie()
        
        # Afficher le r√©sum√©
        if processed_mix or processed_carbone:
            print(f"\n It√©ration #{iteration} termin√©e avec succ√®s")
        else:
            print(f"\n It√©ration #{iteration} termin√©e (aucune nouvelle donn√©e)")
        
        # Attendre 30 secondes
        print(f"\n Prochaine synchronisation dans 30 secondes...")
        time.sleep(30)

except KeyboardInterrupt:
    print("\n\n" + "=" * 70)
    print(" ARR√äT DEMAND√â PAR L'UTILISATEUR")
    print("=" * 70)
    print(f" Total d'it√©rations: {iteration}")
    print(" Arr√™t propre du streaming")
    spark.stop()

except Exception as e:
    print(f"\n ERREUR CRITIQUE: {e}")
    import traceback
    traceback.print_exc()
    spark.stop()
    raise